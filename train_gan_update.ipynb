{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ammar\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\numpy\\_distributor_init.py:30: UserWarning: loaded more than 1 DLL from .libs:\n",
      "c:\\Users\\ammar\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\numpy\\.libs\\libopenblas.EL2C6PLE4ZYW3ECEVIV3OXXGRN2NRFM2.gfortran-win_amd64.dll\n",
      "c:\\Users\\ammar\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\numpy\\.libs\\libopenblas64__v0.3.21-gcc_10_3_0.dll\n",
      "  warnings.warn(\"loaded more than 1 DLL from .libs:\"\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3202, 178, 178)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt # for plotting\n",
    "import numpy as np\n",
    "import torch\n",
    "import math\n",
    "import os\n",
    "\n",
    "class_names = {\n",
    "    0: 'A',\n",
    "    1: 'B',\n",
    "    2: 'C',\n",
    "    3: 'D',\n",
    "    4: 'E',\n",
    "    5: 'F',\n",
    "    6: 'G',\n",
    "    7: 'H',\n",
    "    8: 'I'\n",
    "}\n",
    "\n",
    "transform = transforms.Compose([\n",
    "            # transforms.RandomHorizontalFlip(), # Flip the data horizontally\n",
    "            transforms.ToPILImage(),\n",
    "            transforms.Resize((64, 64)),\n",
    "            transforms.RandomAdjustSharpness(0.25),\n",
    "            transforms.RandomHorizontalFlip(0.5),\n",
    "            transforms.RandomVerticalFlip(0.5),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=(0.5,0.5,0.5), std=(0.5,0.5,0.5))\n",
    "        ])\n",
    "\n",
    "# Load the data with all of the classes\n",
    "root = 'data/birds1/'\n",
    "data_dirs = os.listdir(root)\n",
    "data_dirs = np.array(data_dirs)\n",
    "data_dirs = data_dirs.flatten()\n",
    "\n",
    "roots = np.array([root] * len(data_dirs))\n",
    "data_dirs = np.core.defchararray.add(roots, data_dirs)\n",
    "\n",
    "# data_list = np.array(data.imgs)\n",
    "\n",
    "# Get the indices for the train/val/test split of 75/15/15\n",
    "train_split = math.floor(0.9 * len(data_dirs))\n",
    "val_split = math.ceil(0.05 * len(data_dirs))\n",
    "test_split = val_split\n",
    "\n",
    "# print(train_split + val_split + test_split, len(data_arr))\n",
    "\n",
    "# Make sure the splits are correct\n",
    "assert train_split + val_split + test_split == len(data_dirs)\n",
    "\n",
    "# Split the dataset randomely\n",
    "generator = torch.Generator().manual_seed(1)\n",
    "train, val, test = torch.utils.data.random_split(data_dirs, [train_split, val_split, test_split], generator=generator)\n",
    "\n",
    "len(train), len(val), len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "class GenData(torch.utils.data.Dataset):\n",
    "    '''\n",
    "        Data set class to store the feature maps\n",
    "    \n",
    "    '''\n",
    "\n",
    "    def __init__(self, in_data, transform=None):\n",
    "        # data = np.array(in_data)\n",
    "        self.input_dirs = in_data\n",
    "        # self.labels = data[:, 1]\n",
    "        self.labels = []\n",
    "        self.input_transform = transform[0]\n",
    "        self.transform = transform[1]\n",
    "        self.resize_transform = transforms.Compose([\n",
    "                                    transforms.ToPILImage(),\n",
    "                                    transforms.Resize(128)\n",
    "                                ])\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_dirs)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        input_dir = self.input_dirs[idx]\n",
    "\n",
    "        # Load the data\n",
    "        inputs = cv2.imread(input_dir)\n",
    "        inputs = cv2.cvtColor(inputs, cv2.COLOR_BGR2RGB)\n",
    "        # inputs = inputs.swapaxes(0, 2)\n",
    "\n",
    "        # labels = (int)(self.labels[idx])\n",
    "\n",
    "        if self.transform:\n",
    "            inputs = self.input_transform(inputs)\n",
    "            transformed = self.transform(inputs)\n",
    "\n",
    "            # labels = self.input_transform(labels)\n",
    "\n",
    "            return inputs, transformed, inputs\n",
    "        else:\n",
    "            return inputs, inputs, inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data loaders\n",
    "def get_data_loaders(batch_size=1):\n",
    "    \n",
    "    train_data = GenData(data_dirs[train.indices], transform=(transform, transform))\n",
    "    val_data = GenData(data_dirs[val.indices], transform=(transform, transform))\n",
    "    test_data = GenData(data_dirs[test.indices], transform=(transform, transform))\n",
    "\n",
    "    train_dataloader = torch.utils.data.DataLoader(train_data, batch_size=batch_size)\n",
    "    val_dataloader = torch.utils.data.DataLoader(val_data, batch_size=batch_size)\n",
    "    test_dataloader = torch.utils.data.DataLoader(test_data, batch_size=batch_size)\n",
    "\n",
    "    return train_dataloader, val_dataloader, test_dataloader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "'''Code to visualize the data in the training set'''\n",
    "\n",
    "import torchvision\n",
    "\n",
    "def imsave(inp, savename=None, title=None):\n",
    "    \"\"\"Display image for Tensor.\"\"\"\n",
    "    inp = torchvision.utils.make_grid(inp)\n",
    "    inp = inp.detach().cpu().numpy().transpose((1, 2, 0))\n",
    "    mean = np.array([0.5, 0.5, 0.5])\n",
    "    std = np.array([0.5, 0.5, 0.5])\n",
    "    inp = std * inp + mean\n",
    "    inp = np.clip(inp, 0, 1)\n",
    "    plt.imsave(f'pics/{savename}.png', inp)\n",
    "    # if title is not None:\n",
    "    #     plt.title(title)\n",
    "    # plt.pause(0.001)  # pause a bit so that plots are updated\n",
    "\n",
    "def imshow(inp, title=None):\n",
    "    \"\"\"Display image for Tensor.\"\"\"\n",
    "    inp = inp.numpy().transpose((1, 2, 0))\n",
    "    mean = np.array([0.5, 0.5, 0.5])\n",
    "    std = np.array([0.5, 0.5, 0.5])\n",
    "    inp = std * inp + mean\n",
    "    inp = np.clip(inp, 0, 1)\n",
    "    plt.imshow(inp)\n",
    "    if title is not None:\n",
    "        plt.title(title)\n",
    "    plt.pause(0.001)  # pause a bit so that plots are updated\n",
    "# Get a batch of training data\n",
    "train_dataloader, _, _ = get_data_loaders(8)\n",
    "inputs, _, _ = next(iter(train_dataloader))\n",
    "\n",
    "# Make a grid from batch\n",
    "out = torchvision.utils.make_grid(inputs)\n",
    "# imshow(out, title=[class_names[x.item()] for x in classes])\n",
    "imshow(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Helper functions (based off of previous labs)\n",
    "'''\n",
    "def get_model_name(name, batch_size, learning_rate, momentum, epoch):\n",
    "    \"\"\" Generate a name for the model consisting of all the hyperparameter values\n",
    "\n",
    "    Args:\n",
    "        config: Configuration object containing the hyperparameters\n",
    "    Returns:\n",
    "        path: A string with the hyperparameter name and value concatenated\n",
    "    \"\"\"\n",
    "    path = \"model_{0}_bs{1}_lr{2}_m{3}_epoch{4}\".format(name,\n",
    "                                                   batch_size,\n",
    "                                                   learning_rate,\n",
    "                                                   momentum,\n",
    "                                                   epoch)\n",
    "    return path\n",
    "\n",
    "\n",
    "# Training Curve\n",
    "def plot_loss_curve(train_gen_loss, train_disc_loss):\n",
    "    \"\"\" Plots the training curve for a model run, given the csv files\n",
    "    containing the train/validation error/loss.\n",
    "\n",
    "    Args:\n",
    "        path: The base path of the csv files produced during training\n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    n = len(train_gen_loss) # number of iterations\n",
    "    plt.title(\"Generator Loss vs Discriminator Loss\")\n",
    "    plt.plot(range(1,n+1), train_gen_loss, label=\"Gen Loss\")\n",
    "    plt.plot(range(1,n+1), train_disc_loss, label=\"Disc Loss\")\n",
    "    plt.xlabel(\"Iteration\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend(loc='best')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAN(nn.Module):\n",
    "    def __init__(self, latentDim = 100, featureGen = 64, featureDisc = 64):\n",
    "        super().__init__()\n",
    "\n",
    "        self.generator == nn.Sequential(\n",
    "            # latentDim x 1 x 1 -> featureGen * 8 x 4 x 4\n",
    "            nn.ConvTranspose2d(latentDim, featureGen * 8, kernel_size = 4,\n",
    "                                stride = 1, padding = 0, bias = False)\n",
    "            nn.BatchNorm2d(feautreGen * 8),\n",
    "            nn.LeakyReLU(negative_slope = 0.01, inplace = True),\n",
    "\n",
    "            # featureGen * 8 x 4 x 4 -> featureGen * 4 x 8 x 8\n",
    "            nn.ConvTranspose2d(featureGen * 8, featureGen * 4, kernel_size = 4,\n",
    "                                stride = 2, padding = 1, bias = False)\n",
    "            nn.BatchNorm2d(feautreGen * 4),\n",
    "            nn.LeakyReLU(negative_slope = 0.01, inplace = True),\n",
    "\n",
    "            #  featureGen * 4 x 8 x 8 -> featureGen * 2 x 16 x 16\n",
    "            nn.ConvTranspose2d(featureGen * 4, featureGen * 2, kernel_size = 4,\n",
    "                                stride = 2, padding = 1, bias = False)\n",
    "            nn.BatchNorm2d(feautreGen * 2),\n",
    "            nn.LeakyReLU(negative_slope = 0.01, inplace = True),\n",
    "\n",
    "            # featureGen * 2 x 16 x 16 -> featureGen x 32 x 32\n",
    "            nn.ConvTranspose2d(featureGen * 2, featureGen, kernel_size = 4,\n",
    "                                stride = 2, padding = 1, bias = False)\n",
    "            nn.BatchNorm2d(feautreGen),\n",
    "            nn.LeakyReLU(negative_slope = 0.01, inplace = True),\n",
    "\n",
    "            # featureGen x 32 x 32 -> 3 x 64 x 64\n",
    "            nn.ConvTranspose2d(featureGen, 3, kernel_size = 4,\n",
    "                                stride = 2, padding = 1, bias = False),\n",
    "        \n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "        self.discriminator = nn.Sequential(\n",
    "            # 3 x 64 x 64 -> featureDisc x 32 x 32\n",
    "            nn.Conv2d(3, featureDisc, kernel_size = 4,\n",
    "                        stride = 2, padding = 1),\n",
    "            nn.LeakyReLU(negative_slope = 0.01, inplace = True),\n",
    "\n",
    "            # featureDisc x 32 x 32 -> featureDisc * 2 x 16 x 16\n",
    "            nn.Conv2d(featureDisc, featureDisc * 2, kernel_size = 4,\n",
    "                        stride = 2, padding = 1),\n",
    "            nn.BacthNorm2d(featureDisc * 2)\n",
    "            nn.LeakyReLU(negative_slope = 0.01, inplace = True),\n",
    "\n",
    "            # featureDisc * 2 x 16 x 16 -> featureDisc * 4 x 8 x 8\n",
    "            nn.Conv2d(featureDisc * 2, featureDisc * 4, kernel_size = 4,\n",
    "                        stride = 2, padding = 1),\n",
    "            nn.BacthNorm2d(featureDisc * 4)\n",
    "            nn.LeakyReLU(negative_slope = 0.01, inplace = True),\n",
    "\n",
    "            # featureDisc * 4 x 8 x 8 -> featureDisc * 8 x 4 x 4\n",
    "            nn.Conv2d(featureDisc * 4, featureDisc * 8, kernel_size = 4,\n",
    "                        stride = 2, padding = 1),\n",
    "            nn.BacthNorm2d(featureDisc * 8)\n",
    "            nn.LeakyReLU(negative_slope = 0.01, inplace = True),\n",
    "\n",
    "            # featureDisc * 8 x 4 x 4 -> 1 x 1 x 1\n",
    "            nn.Conv2d(featureDisc * 8, 1, kernel_size = 4,\n",
    "                    stride = 1, padding = 0),\n",
    "                    \n",
    "            nn.Flatten()\n",
    "        )\n",
    "\n",
    "    def genForward(self, x):\n",
    "        return self.generator(x)\n",
    "\n",
    "    def discForward(self, img):\n",
    "        return self.discriminator(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_discriminator(model, inputs, latent_dim, criterion):\n",
    "  batchSize = inputs.size(0)\n",
    "\n",
    "  real_images = inputs.to(device)\n",
    "  realLabels = torch.ones(batchSize, device = device)\n",
    "\n",
    "  noise = torch.randn(batchSize, latent_dim, 1, 1 device = device)\n",
    "  fake_images = model.genForward(noise)\n",
    "  fakeLabels = torch.zeros(batchSize, device = device)\n",
    "\n",
    "  discPredReal = model.discForward(real_images).view(-1)\n",
    "  realLoss = criterion(discPredReal, realLabels)\n",
    "\n",
    "  discPredFake = model.discForward(fake_images).view(-1)\n",
    "  fakeLoss = criterion(discPredFake, fakeLabels)\n",
    "\n",
    "  discLoss = 0.5 * (realLoss + fakeLoss)\n",
    "  return realLabels, fakeLabels, fake_images, discLoss, discPredReal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_generator(model, inputs, realLabels, criterion):\n",
    "  discPredFake = model.discForward(inputs).view(-1)\n",
    "  genLoss = criterion(discPredFake, realLabels)\n",
    "\n",
    "  return genLoss, discPredFake\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Train Loop\n",
    "'''\n",
    "\n",
    "import tqdm\n",
    "import time\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "OUTPUT_DIR = 'runs/'\n",
    "\n",
    "def train_network(model, latent_dim, device, trainLoader, num_epochs, lr):\n",
    "\n",
    "    # Set manual seed for reproducible results\n",
    "    torch.manual_seed(1)\n",
    "\n",
    "    # Criterion and optimizer\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    # optimizer =  optim.SGD(model.parameters(), lr=lr, momentum=momentum)\n",
    "    gen_optimizer =  optim.Adam(generator.parameters(), lr=lr, weight_decay=1e-3)\n",
    "    disc_optimizer =  optim.Adam(discriminator.parameters(), lr=lr, weight_decay=1e-3)\n",
    "\n",
    "\n",
    "    # Training metrics\n",
    "    train_gen_loss = []\n",
    "    train_disc_loss = []\n",
    "    train_disc_real_acc = []\n",
    "    train_disc_fake_acc = []\n",
    "    newImgs = []\n",
    "\n",
    "    best_train_err = 1000\n",
    "    best_val_err = 1000\n",
    "\n",
    "    ########## SENDING TO CUDA ############\n",
    "    if cuda:\n",
    "        generator = generator.to('cuda:0')\n",
    "        discriminator = discriminator.to('cuda:0')\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Train loop\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        iteration = 0\n",
    "        for imgs, _ in trainLoader: \n",
    "\n",
    "            realLabels, fakeLabels, fake_images, discLoss, discPredReal = train_discriminator(model, imgs, latent_dim, criterion)\n",
    "            discLoss.backward()\n",
    "            disc_optimizer.step()\n",
    "            disc_optimizer.zero_grad()\n",
    "\n",
    "            genLoss, discPredFake = train_generator(model, fake_images, realLabels, criterion)\n",
    "            genLoss.backward()\n",
    "            gen_optimizer.step()\n",
    "            gen_optimizer.zero_grad()\n",
    "                \n",
    "            predReal = torch.where(discPredReal.detach() > 0.0, 1.0, 0.0)\n",
    "            predFake = torch.where(discPredFake.detach() > 0.0, 1.0, 0.0)\n",
    "            accReal = (predReal == realLabels).float().mean() * 100\n",
    "            accFake = (predFake == fakeLabels).float().mean() * 100\n",
    "\n",
    "            train_gen_loss.append(genLoss.item())\n",
    "            train_disc_loss.append(discLoss.item())\n",
    "            train_disc_real_acc.append(accReal.item())\n",
    "            train_disc_fake_acc.append(accFake.item())\n",
    "\n",
    "            iteration += 1\n",
    "\n",
    "            if(iteration % interval == 0):\n",
    "                print(\"Epoch: %03d/%03d | Batch %03d/%03d | Gen/Disc Loss: %.4f/%.4f\"\n",
    "                        % (epoch + 1, num_epochs, iteration, len(trainLoader), \n",
    "                            genLoss.item(), discLoss.item()))\n",
    "        newImgs.append(torchvision.utils.make_grid(fake_images, padding = 2, normalzie = True))\n",
    "        print(\"Time elapsed: %.2f min\" % ((time.time() - start_time) / 60))\n",
    "    \n",
    "    print('Finished Training')\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    print(\"Total time elapsed: {:.2f} seconds\".format(elapsed_time))\n",
    "    \n",
    "    return train_gen_loss, train_disc_loss, train_disc_real_acc, train_disc_fake_acc, newImgs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "gan_model = GAN(latentDim = 100, featureGen = 64, featureDisc = 64)\n",
    "gan_model.name = 'gan'\n",
    "trainLoader, valLoader, testLoader = get_data_loaders(batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/500 [00:19<2:38:15, 19.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Gen Train Loss: 29097403.108580347, Disc Train Loss: 0.2159889082060587 |\n",
      " Gen Val Loss: 377763.0625, Disc Val Loss: 9.593592115682744e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 2/500 [00:37<2:33:18, 18.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Gen Train Loss: 409570.67095588235, Disc Train Loss: 0.0032139749753886692 |\n",
      " Gen Val Loss: 334243.3541666667, Disc Val Loss: 0.00027860404225066304\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 3/500 [00:55<2:33:09, 18.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Gen Train Loss: 391751.4074754902, Disc Train Loss: 0.00030081143703165593 |\n",
      " Gen Val Loss: 330440.3541666667, Disc Val Loss: 7.941416697576642e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 4/500 [01:21<2:56:39, 21.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: Gen Train Loss: 5108638.6875, Disc Train Loss: 0.0001561044112296419 |\n",
      " Gen Val Loss: 376984.9166666667, Disc Val Loss: 4.3597688393977783e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 5/500 [01:43<2:59:18, 21.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: Gen Train Loss: 572164.3400735294, Disc Train Loss: 0.040952617627128735 |\n",
      " Gen Val Loss: 406148.4479166667, Disc Val Loss: 8.47627295759897e-08\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 6/500 [02:04<2:55:36, 21.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Gen Train Loss: 390803.6574754902, Disc Train Loss: 0.0002676265030890237 |\n",
      " Gen Val Loss: 1162723.5208333333, Disc Val Loss: 5.694873692618785e-07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏         | 7/500 [02:21<2:45:11, 20.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: Gen Train Loss: 616196.2849264706, Disc Train Loss: 2.1311554712304305e-05 |\n",
      " Gen Val Loss: 392670.0833333333, Disc Val Loss: 7.044515465774263e-07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 8/500 [02:41<2:44:20, 20.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: Gen Train Loss: 446339.92156862747, Disc Train Loss: 0.00032808773251720896 |\n",
      " Gen Val Loss: 434258.5, Disc Val Loss: 8.269674071925692e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 9/500 [02:59<2:38:58, 19.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: Gen Train Loss: 1036980.0171568628, Disc Train Loss: 2.4211312009748664e-05 |\n",
      " Gen Val Loss: 448823.2916666667, Disc Val Loss: 2.2976829010682803e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 10/500 [03:19<2:39:19, 19.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: Gen Train Loss: 486825.7708333333, Disc Train Loss: 0.035771987815928254 |\n",
      " Gen Val Loss: 455815.6666666667, Disc Val Loss: 0.17665529747804007\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 11/500 [03:41<2:44:15, 20.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: Gen Train Loss: 195007398.40931374, Disc Train Loss: 0.01291850242848216 |\n",
      " Gen Val Loss: 526871.5104166666, Disc Val Loss: 0.00020991348719689995\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 12/500 [03:58<2:37:07, 19.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11: Gen Train Loss: 1710331.0484068627, Disc Train Loss: 0.000159203049477975 |\n",
      " Gen Val Loss: 498729.4791666667, Disc Val Loss: 0.00012344693338188031\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 13/500 [04:15<2:32:00, 18.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12: Gen Train Loss: 27176594.52512255, Disc Train Loss: 9.38843142791359e-05 |\n",
      " Gen Val Loss: 1058627.6666666667, Disc Val Loss: 7.046164197769637e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 14/500 [04:35<2:33:29, 18.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13: Gen Train Loss: 7174549.711397059, Disc Train Loss: 6.278934483370725e-05 |\n",
      " Gen Val Loss: 532609.6458333334, Disc Val Loss: 6.159655458759516e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 15/500 [04:57<2:39:56, 19.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14: Gen Train Loss: 806296.9865196078, Disc Train Loss: 5.9177147526426384e-05 |\n",
      " Gen Val Loss: 558281.6666666666, Disc Val Loss: 5.728336085060922e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 16/500 [05:14<2:33:32, 19.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15: Gen Train Loss: 21230509095364.68, Disc Train Loss: 5.4102695168123815e-05 |\n",
      " Gen Val Loss: 815174.0833333334, Disc Val Loss: 5.187668405900089e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 17/500 [05:30<2:26:42, 18.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16: Gen Train Loss: 1303627.7573529412, Disc Train Loss: 5.272291673463769e-05 |\n",
      " Gen Val Loss: 797225.4166666666, Disc Val Loss: 5.390795073860014e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▎         | 18/500 [05:47<2:22:45, 17.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17: Gen Train Loss: 27733492259720.484, Disc Train Loss: 5.789261879673337e-05 |\n",
      " Gen Val Loss: 942196.9583333334, Disc Val Loss: 6.13378676158997e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 19/500 [06:04<2:20:54, 17.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18: Gen Train Loss: 12774121.799019609, Disc Train Loss: 6.427892759686573e-05 |\n",
      " Gen Val Loss: 929456.3541666666, Disc Val Loss: 6.921681779203936e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 20/500 [06:22<2:20:40, 17.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: Gen Train Loss: 1058758564.7193627, Disc Train Loss: 7.652792598545442e-05 |\n",
      " Gen Val Loss: 502561453.8333333, Disc Val Loss: 8.387841080548242e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 21/500 [06:40<2:21:13, 17.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20: Gen Train Loss: 15733386.045343136, Disc Train Loss: 9.136705250734939e-05 |\n",
      " Gen Val Loss: 3524513.6875, Disc Val Loss: 0.00010103577854655062\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 22/500 [06:56<2:17:15, 17.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21: Gen Train Loss: 4764024233134.692, Disc Train Loss: 0.00010861932084891105 |\n",
      " Gen Val Loss: 1099109.125, Disc Val Loss: 0.00012193421328750749\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▍         | 23/500 [07:11<2:11:06, 16.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22: Gen Train Loss: 88469069.10294117, Disc Train Loss: 0.0001293059350584871 |\n",
      " Gen Val Loss: 1980894.5416666667, Disc Val Loss: 0.00013549501212158552\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▍         | 24/500 [07:27<2:10:22, 16.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23: Gen Train Loss: 1192502.5980392157, Disc Train Loss: 0.00015112852983578456 |\n",
      " Gen Val Loss: 1147048.25, Disc Val Loss: 0.00017229879934651157\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 25/500 [07:44<2:10:44, 16.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24: Gen Train Loss: 1234986628.9264705, Disc Train Loss: 0.00018129859567053762 |\n",
      " Gen Val Loss: 1161754.4583333333, Disc Val Loss: 0.0001825705743006741\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 26/500 [08:05<2:21:22, 17.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25: Gen Train Loss: 24024587.531862747, Disc Train Loss: 0.00020274616134188632 |\n",
      " Gen Val Loss: 417996412.125, Disc Val Loss: 0.0002168903301935643\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 27/500 [08:26<2:28:18, 18.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26: Gen Train Loss: 9896757016.093138, Disc Train Loss: 0.00023676105917604895 |\n",
      " Gen Val Loss: 1294206.9583333333, Disc Val Loss: 0.00022277157889523855\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 28/500 [08:48<2:35:24, 19.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27: Gen Train Loss: 137684113237.98038, Disc Train Loss: 0.0002438117480065272 |\n",
      " Gen Val Loss: 1134868.875, Disc Val Loss: 0.000294151104753837\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 29/500 [09:09<2:38:27, 20.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28: Gen Train Loss: 7844890603.169118, Disc Train Loss: 0.00025406775553254226 |\n",
      " Gen Val Loss: 7144664.875, Disc Val Loss: 0.0002730813090844701\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 30/500 [09:27<2:33:46, 19.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29: Gen Train Loss: 2.1765278044342296e+16, Disc Train Loss: 0.0002721665037896338 |\n",
      " Gen Val Loss: 1200454.375, Disc Val Loss: 0.00025650237027245265\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 31/500 [09:46<2:31:05, 19.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30: Gen Train Loss: 45605660.10294118, Disc Train Loss: 0.000272463818820303 |\n",
      " Gen Val Loss: 125001224.79166667, Disc Val Loss: 0.00034018631170814234\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▋         | 32/500 [10:06<2:33:55, 19.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31: Gen Train Loss: 10411776581.799019, Disc Train Loss: 0.0002917988544247825 |\n",
      " Gen Val Loss: 38672847677.541664, Disc Val Loss: 0.00028595417582740385\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 33/500 [10:25<2:31:51, 19.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32: Gen Train Loss: 17077502084.72304, Disc Train Loss: 0.0002901413867149648 |\n",
      " Gen Val Loss: 1631775.4583333333, Disc Val Loss: 0.0002951412849749128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 34/500 [10:46<2:33:39, 19.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33: Gen Train Loss: 84774508.2254902, Disc Train Loss: 0.00029279133042448436 |\n",
      " Gen Val Loss: 1328999.7083333333, Disc Val Loss: 0.00029762464691884816\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 35/500 [11:07<2:37:27, 20.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34: Gen Train Loss: 8373255026960.044, Disc Train Loss: 0.00029532366384313823 |\n",
      " Gen Val Loss: 1378749.0833333333, Disc Val Loss: 0.0002751498056265215\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 36/500 [11:29<2:40:53, 20.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35: Gen Train Loss: 448922082144.799, Disc Train Loss: 0.00028778535631193106 |\n",
      " Gen Val Loss: 15925170.583333334, Disc Val Loss: 0.0002940804891598721\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 37/500 [11:48<2:34:53, 20.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36: Gen Train Loss: 328559399.04901963, Disc Train Loss: 0.0002929111594544249 |\n",
      " Gen Val Loss: 1450871.8333333333, Disc Val Loss: 0.00031334229667360586\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 38/500 [12:06<2:30:51, 19.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37: Gen Train Loss: 1.2858630627586002e+17, Disc Train Loss: 0.0002988153416891674 |\n",
      " Gen Val Loss: 5721670966.583333, Disc Val Loss: 0.0003011114022228867\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 39/500 [12:25<2:27:35, 19.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38: Gen Train Loss: 19542505984.414215, Disc Train Loss: 0.00029595494361630843 |\n",
      " Gen Val Loss: 1802094.125, Disc Val Loss: 0.00031273656835158664\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 40/500 [12:43<2:24:59, 18.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39: Gen Train Loss: 3995265471.497549, Disc Train Loss: 0.0002987671765617515 |\n",
      " Gen Val Loss: 1815431.625, Disc Val Loss: 0.0002842631268625458\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 41/500 [13:01<2:23:34, 18.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40: Gen Train Loss: 81467995511.79167, Disc Train Loss: 0.00029897744603016795 |\n",
      " Gen Val Loss: 1758940.875, Disc Val Loss: 0.00033573078690096736\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 42/500 [13:20<2:23:11, 18.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 41: Gen Train Loss: 4832734520.063725, Disc Train Loss: 0.00029432942335238203 |\n",
      " Gen Val Loss: 1827688.125, Disc Val Loss: 0.00030896432387332123\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▊         | 43/500 [13:38<2:21:36, 18.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 42: Gen Train Loss: 7891813.946078432, Disc Train Loss: 0.0002979314018625255 |\n",
      " Gen Val Loss: 238705200.54166666, Disc Val Loss: 0.00029340631832989555\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 44/500 [13:56<2:20:17, 18.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 43: Gen Train Loss: 1897576.6789215687, Disc Train Loss: 0.00029874685295747924 |\n",
      " Gen Val Loss: 1836810.75, Disc Val Loss: 0.0002984727907460183\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 45/500 [14:17<2:26:13, 19.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 44: Gen Train Loss: 127369800375885.08, Disc Train Loss: 0.0002935524800491026 |\n",
      " Gen Val Loss: 1864748.2083333333, Disc Val Loss: 0.0002880167739931494\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 46/500 [14:37<2:25:58, 19.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45: Gen Train Loss: 19966205.475490198, Disc Train Loss: 0.01433354589108474 |\n",
      " Gen Val Loss: 1829775.875, Disc Val Loss: 2.577597479103133e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 47/500 [14:55<2:23:03, 18.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 46: Gen Train Loss: 12061086327.705883, Disc Train Loss: 4.323963652246212e-06 |\n",
      " Gen Val Loss: 1804364.25, Disc Val Loss: 3.9888562544850475e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|▉         | 48/500 [15:14<2:22:52, 18.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47: Gen Train Loss: 406328507.3872549, Disc Train Loss: 1.100099608069571e-05 |\n",
      " Gen Val Loss: 257917575.375, Disc Val Loss: 2.420824481911647e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|▉         | 49/500 [15:34<2:23:56, 19.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48: Gen Train Loss: 548246092.137255, Disc Train Loss: 4.189520551139425e-05 |\n",
      " Gen Val Loss: 1820055.625, Disc Val Loss: 6.212829612195492e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 50/500 [15:52<2:22:08, 18.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49: Gen Train Loss: 257007353.49264705, Disc Train Loss: 6.543074285967604e-05 |\n",
      " Gen Val Loss: 1864993.0, Disc Val Loss: 7.08891335913601e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 51/500 [16:12<2:23:12, 19.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50: Gen Train Loss: 2.3651376062626343e+20, Disc Train Loss: 7.976885050690423e-05 |\n",
      " Gen Val Loss: 1999410.75, Disc Val Loss: 8.744591226180394e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 52/500 [16:30<2:20:22, 18.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 51: Gen Train Loss: 1148937206.497549, Disc Train Loss: 9.703436052066037e-05 |\n",
      " Gen Val Loss: 1887903.125, Disc Val Loss: 0.00010606024200872828\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 53/500 [16:48<2:18:31, 18.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 52: Gen Train Loss: 3607435.125, Disc Train Loss: 0.000151586313115652 |\n",
      " Gen Val Loss: 1862783.375, Disc Val Loss: 0.0006006414963242909\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 54/500 [17:06<2:17:36, 18.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 53: Gen Train Loss: 6275993257.82598, Disc Train Loss: 0.00018866069648556375 |\n",
      " Gen Val Loss: 1887070.8333333333, Disc Val Loss: 0.0001670114870648831\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 55/500 [17:24<2:16:37, 18.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 54: Gen Train Loss: 55482061720.73284, Disc Train Loss: 0.00017524411868733152 |\n",
      " Gen Val Loss: 1918998.7083333333, Disc Val Loss: 0.0002054367990543445\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 56/500 [17:44<2:18:23, 18.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 55: Gen Train Loss: 196314291.88480392, Disc Train Loss: 0.0002151239162890752 |\n",
      " Gen Val Loss: 1967304.75, Disc Val Loss: 0.00023060178985664\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█▏        | 57/500 [18:02<2:17:03, 18.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 56: Gen Train Loss: 8.5342877630672e+24, Disc Train Loss: 0.0002472872934852015 |\n",
      " Gen Val Loss: 2010046.9166666667, Disc Val Loss: 0.0002666975681980451\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 58/500 [18:22<2:19:38, 18.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57: Gen Train Loss: 347949208.27205884, Disc Train Loss: 0.00031201570242296394 |\n",
      " Gen Val Loss: 2066221.6666666667, Disc Val Loss: 0.000261278076019759\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 59/500 [18:41<2:19:36, 18.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58: Gen Train Loss: 1.2578396221564712e+23, Disc Train Loss: 0.00024156358124980448 |\n",
      " Gen Val Loss: 2042932.3333333333, Disc Val Loss: 0.0003368485583147655\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 60/500 [18:59<2:18:07, 18.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59: Gen Train Loss: 20274457427.887257, Disc Train Loss: 0.004760977820100665 |\n",
      " Gen Val Loss: 2363228.25, Disc Val Loss: 2.2587170600891113\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 61/500 [19:19<2:19:05, 19.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 60: Gen Train Loss: 32068779.79411765, Disc Train Loss: 0.04698720576227382 |\n",
      " Gen Val Loss: 2129286.75, Disc Val Loss: 1.1071500011894386e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 62/500 [19:38<2:19:28, 19.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 61: Gen Train Loss: 1.2633009343266444e+21, Disc Train Loss: 4.234942398219502e-05 |\n",
      " Gen Val Loss: 2144872.25, Disc Val Loss: 7.90621464451154e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 63/500 [19:56<2:16:43, 18.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 62: Gen Train Loss: 14265884938.617647, Disc Train Loss: 0.00010482949480373303 |\n",
      " Gen Val Loss: 2143742.8333333335, Disc Val Loss: 0.00014360823358098665\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 63/500 [20:07<2:19:38, 19.17s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\ammar\\Documents\\Year2\\aps360\\APS360\\train_gan.ipynb Cell 14\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ammar/Documents/Year2/aps360/APS360/train_gan.ipynb#X16sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m m \u001b[39m=\u001b[39m \u001b[39m0.9\u001b[39m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ammar/Documents/Year2/aps360/APS360/train_gan.ipynb#X16sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m epochs \u001b[39m=\u001b[39m \u001b[39m500\u001b[39m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/ammar/Documents/Year2/aps360/APS360/train_gan.ipynb#X16sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m train_network(gen_model, disc_model, batch_size\u001b[39m=\u001b[39;49mbatch, lr\u001b[39m=\u001b[39;49mlr, momentum\u001b[39m=\u001b[39;49mm, num_epochs\u001b[39m=\u001b[39;49mepochs, cuda\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ammar/Documents/Year2/aps360/APS360/train_gan.ipynb#X16sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m model_path \u001b[39m=\u001b[39m  OUTPUT_DIR \u001b[39m+\u001b[39m get_model_name(gen_model\u001b[39m.\u001b[39mname, batch, lr, m, epochs\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ammar/Documents/Year2/aps360/APS360/train_gan.ipynb#X16sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m plot_training_curve(model_path)\n",
      "\u001b[1;32mc:\\Users\\ammar\\Documents\\Year2\\aps360\\APS360\\train_gan.ipynb Cell 14\u001b[0m in \u001b[0;36mtrain_network\u001b[1;34m(generator, discriminator, batch_size, lr, momentum, num_epochs, cuda, loader, alex, sheduler)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ammar/Documents/Year2/aps360/APS360/train_gan.ipynb#X16sZmlsZQ%3D%3D?line=69'>70</a>\u001b[0m     inputs \u001b[39m=\u001b[39m inputs\u001b[39m.\u001b[39mto(\u001b[39m'\u001b[39m\u001b[39mcuda:0\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ammar/Documents/Year2/aps360/APS360/train_gan.ipynb#X16sZmlsZQ%3D%3D?line=70'>71</a>\u001b[0m     transformed \u001b[39m=\u001b[39m transformed\u001b[39m.\u001b[39mto(\u001b[39m'\u001b[39m\u001b[39mcuda:0\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/ammar/Documents/Year2/aps360/APS360/train_gan.ipynb#X16sZmlsZQ%3D%3D?line=72'>73</a>\u001b[0m _, disc_loss \u001b[39m=\u001b[39m train_discriminator(discriminator, generator, inputs, generator\u001b[39m.\u001b[39;49mlatent_dim, criterion \u001b[39m=\u001b[39;49m criterion)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ammar/Documents/Year2/aps360/APS360/train_gan.ipynb#X16sZmlsZQ%3D%3D?line=73'>74</a>\u001b[0m \u001b[39mif\u001b[39;00m mode \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ammar/Documents/Year2/aps360/APS360/train_gan.ipynb#X16sZmlsZQ%3D%3D?line=74'>75</a>\u001b[0m     disc_loss\u001b[39m.\u001b[39mbackward()\n",
      "\u001b[1;32mc:\\Users\\ammar\\Documents\\Year2\\aps360\\APS360\\train_gan.ipynb Cell 14\u001b[0m in \u001b[0;36mtrain_discriminator\u001b[1;34m(discriminator, generator, inputs, latent_dim, criterion)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ammar/Documents/Year2/aps360/APS360/train_gan.ipynb#X16sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m batch_size \u001b[39m=\u001b[39m inputs\u001b[39m.\u001b[39msize(\u001b[39m0\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ammar/Documents/Year2/aps360/APS360/train_gan.ipynb#X16sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m noise \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mrandn(batch_size, latent_dim)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/ammar/Documents/Year2/aps360/APS360/train_gan.ipynb#X16sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m fake_images, _, _, _ \u001b[39m=\u001b[39m generator(inputs)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ammar/Documents/Year2/aps360/APS360/train_gan.ipynb#X16sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m inputs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat([inputs, fake_images])\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ammar/Documents/Year2/aps360/APS360/train_gan.ipynb#X16sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m labels \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat([torch\u001b[39m.\u001b[39mzeros(batch_size), \u001b[39m# Real\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ammar/Documents/Year2/aps360/APS360/train_gan.ipynb#X16sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m torch\u001b[39m.\u001b[39mones(batch_size)]) \u001b[39m# Fake\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ammar\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1106\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1107\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1108\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1111\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32mc:\\Users\\ammar\\Documents\\Year2\\aps360\\APS360\\train_gan.ipynb Cell 14\u001b[0m in \u001b[0;36mGenerator.forward\u001b[1;34m(self, input, cuda, **kwargs)\u001b[0m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/ammar/Documents/Year2/aps360/APS360/train_gan.ipynb#X16sZmlsZQ%3D%3D?line=213'>214</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor, cuda\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m List[Tensor]:\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/Users/ammar/Documents/Year2/aps360/APS360/train_gan.ipynb#X16sZmlsZQ%3D%3D?line=214'>215</a>\u001b[0m     mu, log_var \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencode(\u001b[39minput\u001b[39;49m)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/ammar/Documents/Year2/aps360/APS360/train_gan.ipynb#X16sZmlsZQ%3D%3D?line=215'>216</a>\u001b[0m     z \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreparameterize(mu, log_var)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/ammar/Documents/Year2/aps360/APS360/train_gan.ipynb#X16sZmlsZQ%3D%3D?line=216'>217</a>\u001b[0m     \u001b[39m# noise = torch.randn(batch_size, 100)\u001b[39;00m\n",
      "\u001b[1;32mc:\\Users\\ammar\\Documents\\Year2\\aps360\\APS360\\train_gan.ipynb Cell 14\u001b[0m in \u001b[0;36mGenerator.encode\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/ammar/Documents/Year2/aps360/APS360/train_gan.ipynb#X16sZmlsZQ%3D%3D?line=144'>145</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mencode\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m List[Tensor]:\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/ammar/Documents/Year2/aps360/APS360/train_gan.ipynb#X16sZmlsZQ%3D%3D?line=145'>146</a>\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/ammar/Documents/Year2/aps360/APS360/train_gan.ipynb#X16sZmlsZQ%3D%3D?line=146'>147</a>\u001b[0m \u001b[39m    Encodes the input by passing through the encoder network\u001b[39;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/ammar/Documents/Year2/aps360/APS360/train_gan.ipynb#X16sZmlsZQ%3D%3D?line=147'>148</a>\u001b[0m \u001b[39m    and returns the latent codes.\u001b[39;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/ammar/Documents/Year2/aps360/APS360/train_gan.ipynb#X16sZmlsZQ%3D%3D?line=148'>149</a>\u001b[0m \u001b[39m    :param input: (Tensor) Input tensor to encoder [N x C x H x W]\u001b[39;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/ammar/Documents/Year2/aps360/APS360/train_gan.ipynb#X16sZmlsZQ%3D%3D?line=149'>150</a>\u001b[0m \u001b[39m    :return: (Tensor) List of latent codes\u001b[39;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/ammar/Documents/Year2/aps360/APS360/train_gan.ipynb#X16sZmlsZQ%3D%3D?line=150'>151</a>\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/Users/ammar/Documents/Year2/aps360/APS360/train_gan.ipynb#X16sZmlsZQ%3D%3D?line=151'>152</a>\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(\u001b[39minput\u001b[39;49m)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/ammar/Documents/Year2/aps360/APS360/train_gan.ipynb#X16sZmlsZQ%3D%3D?line=152'>153</a>\u001b[0m     result \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mflatten(result, start_dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/ammar/Documents/Year2/aps360/APS360/train_gan.ipynb#X16sZmlsZQ%3D%3D?line=155'>156</a>\u001b[0m     \u001b[39m# Split the result into mu and var components\u001b[39;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/ammar/Documents/Year2/aps360/APS360/train_gan.ipynb#X16sZmlsZQ%3D%3D?line=156'>157</a>\u001b[0m     \u001b[39m# of the latent Gaussian distribution\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ammar\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1106\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1107\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1108\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1111\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\ammar\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\container.py:141\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    139\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[0;32m    140\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[1;32m--> 141\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[0;32m    142\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\ammar\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1106\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1107\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1108\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1111\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\ammar\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\container.py:141\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    139\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[0;32m    140\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[1;32m--> 141\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[0;32m    142\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\ammar\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1106\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1107\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1108\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1111\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\ammar\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\conv.py:447\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    446\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 447\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conv_forward(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "File \u001b[1;32mc:\\Users\\ammar\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\conv.py:443\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    439\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m    440\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv2d(F\u001b[39m.\u001b[39mpad(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode),\n\u001b[0;32m    441\u001b[0m                     weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[0;32m    442\u001b[0m                     _pair(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n\u001b[1;32m--> 443\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv2d(\u001b[39minput\u001b[39;49m, weight, bias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride,\n\u001b[0;32m    444\u001b[0m                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "batch = 64\n",
    "lr = 0.007\n",
    "m = 0.9\n",
    "epochs = 500\n",
    "\n",
    "train_gen_loss, train_disc_loss, train_disc_real_acc, train_disc_fake_acc, newImgs =\n",
    "train_network(gan_model, 100, device = DEVICE, trainLoader, num_epochs = epochs, lr = lr)\n",
    "model_path =  OUTPUT_DIR + get_model_name(gan_model.name, batch, lr, m, epochs-1)\n",
    "plot_training_curve(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\ammar\\Documents\\Year2\\aps360\\APS360\\train_gan.ipynb Cell 15\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ammar/Documents/Year2/aps360/APS360/train_gan.ipynb#X20sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m img, _, _ \u001b[39m=\u001b[39m data\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ammar/Documents/Year2/aps360/APS360/train_gan.ipynb#X20sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m img \u001b[39m=\u001b[39m img\u001b[39m.\u001b[39mto(\u001b[39m'\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/ammar/Documents/Year2/aps360/APS360/train_gan.ipynb#X20sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m imshow(img[\u001b[39m0\u001b[39;49m])\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ammar/Documents/Year2/aps360/APS360/train_gan.ipynb#X20sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m out \u001b[39m=\u001b[39m gen_model(img)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ammar/Documents/Year2/aps360/APS360/train_gan.ipynb#X20sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m out \u001b[39m=\u001b[39m out[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mcpu()\n",
      "\u001b[1;32mc:\\Users\\ammar\\Documents\\Year2\\aps360\\APS360\\train_gan.ipynb Cell 15\u001b[0m in \u001b[0;36mimshow\u001b[1;34m(inp, title)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ammar/Documents/Year2/aps360/APS360/train_gan.ipynb#X20sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mimshow\u001b[39m(inp, title\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ammar/Documents/Year2/aps360/APS360/train_gan.ipynb#X20sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m     \u001b[39m\"\"\"Display image for Tensor.\"\"\"\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/ammar/Documents/Year2/aps360/APS360/train_gan.ipynb#X20sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m     inp \u001b[39m=\u001b[39m inp\u001b[39m.\u001b[39;49mnumpy()\u001b[39m.\u001b[39mtranspose((\u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m, \u001b[39m0\u001b[39m))\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ammar/Documents/Year2/aps360/APS360/train_gan.ipynb#X20sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m     mean \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray([\u001b[39m0.5\u001b[39m, \u001b[39m0.5\u001b[39m, \u001b[39m0.5\u001b[39m])\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ammar/Documents/Year2/aps360/APS360/train_gan.ipynb#X20sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m     std \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray([\u001b[39m0.5\u001b[39m, \u001b[39m0.5\u001b[39m, \u001b[39m0.5\u001b[39m])\n",
      "\u001b[1;31mTypeError\u001b[0m: can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first."
     ]
    }
   ],
   "source": [
    "interval = 5\n",
    "\n",
    "for x in range(0, epochs, interval):\n",
    "    plt.figure(figsize = (8, 8))\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(\"f'Generated Images at Epoch {x}\")\n",
    "    plt.imshow(np.transpose(newImgs[x], (1, 2, 0)))\n",
    "    plt.show()\n",
    "\n",
    "plt.figure(figsize = (8, 8))\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"f'Generated Images at Last Epoch}\")\n",
    "plt.imshow(np.transpose(newImgs[-1], (1, 2, 0)))\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
