{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "JrA2AidMUUzr"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\ammar\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\numpy\\_distributor_init.py:30: UserWarning: loaded more than 1 DLL from .libs:\n",
            "c:\\Users\\ammar\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\numpy\\.libs\\libopenblas.EL2C6PLE4ZYW3ECEVIV3OXXGRN2NRFM2.gfortran-win_amd64.dll\n",
            "c:\\Users\\ammar\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\numpy\\.libs\\libopenblas64__v0.3.21-gcc_10_3_0.dll\n",
            "  warnings.warn(\"loaded more than 1 DLL from .libs:\"\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import PIL\n",
        "import urllib\n",
        "from torchvision import datasets, transforms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fzeq1W_vletN",
        "outputId": "04ab37d2-c87b-4a5f-8015-d630e845cfe2"
      },
      "outputs": [],
      "source": [
        "mnist_data = datasets.MNIST('../data', train=True, download=True, transform=transforms.ToTensor())\n",
        "mnist_data = list(mnist_data)\n",
        "mnist_train = mnist_data[:10000]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "7-oAPtorn9C5"
      },
      "outputs": [],
      "source": [
        "criterion = nn.BCELoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "21ntGqdSolH1"
      },
      "outputs": [],
      "source": [
        "img_to_tensor = transforms.ToTensor()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "NArlPo-wI7Xy"
      },
      "outputs": [],
      "source": [
        "class Discriminator(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Discriminator, self).__init__()\n",
        "    self.model = nn.Sequential(\n",
        "    nn.Linear(28*28, 300),\n",
        "    nn.LeakyReLU(0.2),\n",
        "    nn.Linear(300, 100),\n",
        "    nn.LeakyReLU(0.2),\n",
        "    nn.Linear(100, 1),\n",
        "    nn.Sigmoid())\n",
        "  def forward(self, x):\n",
        "    x = x.view(x.size(0), -1)\n",
        "    out = self.model(x)\n",
        "    return out.view(x.size(0))\n",
        "\n",
        "class Generator(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Generator, self).__init__()\n",
        "    self.model = nn.Sequential(\n",
        "    nn.Linear(100, 300),\n",
        "    nn.LeakyReLU(0.2),\n",
        "    nn.Linear(300, 28*28),\n",
        "    nn.Sigmoid())\n",
        "  def forward(self, x):\n",
        "    out = self.model(x).view(x.size(0), 1, 28, 28)\n",
        "    return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "Z7F8hteaSw2T"
      },
      "outputs": [],
      "source": [
        "def train_discriminator(discriminator, generator, images):\n",
        "  # batch_size = img_to_tensor(images).size(0)\n",
        "  batch_size = images.size(0)\n",
        "\n",
        "  noise = torch.randn(batch_size, 100)\n",
        "  fake_images = generator(noise)\n",
        "  inputs = torch.cat([images, fake_images])\n",
        "  labels = torch.cat([torch.zeros(batch_size), # Real\n",
        "  torch.ones(batch_size)]) # Fake\n",
        "  outputs = discriminator(inputs)\n",
        "  loss = criterion(outputs, labels)\n",
        "  return outputs, loss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "x79A8lIAS19W"
      },
      "outputs": [],
      "source": [
        "def train_generator(discriminator, generator, images):\n",
        "  batch_size = images.size(0)\n",
        "  noise = torch.randn(batch_size, 100)\n",
        "  fake_images = generator(noise)\n",
        "  outputs = discriminator(fake_images)\n",
        "  # Only looks at fake outputs\n",
        "  # gets rewarded if we fool the discriminator!\n",
        "  labels = torch.zeros(batch_size)\n",
        "  loss = criterion(outputs, labels)\n",
        "  return fake_images, loss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "1rOogssIWpIL"
      },
      "outputs": [],
      "source": [
        "def train(discriminator, generator, lr, epochs, train_loader  ):\n",
        "  torch.manual_seed(42)\n",
        "  optimizerDisc = optim.Adam(discriminator.parameters(), lr = lr)\n",
        "  optimizerGen = optim.Adam(generator.parameters(), lr = lr)\n",
        "\n",
        "  Gtrain_acc, Gval_acc, iters, Gtrain_loss, Gval_loss = [], [], [], [], []\n",
        "  Dtrain_acc, Dval_acc, iters, Dtrain_loss, Dval_loss = [], [], [], [], []\n",
        "  genImgs = []\n",
        "\n",
        "  start_time = time.time()\n",
        "  for epoch in range(epochs):\n",
        "    Gtotal_train_loss = 0.0\n",
        "    Dtotal_train_loss = 0.0\n",
        "    iteration = 0\n",
        "    for imgs, __ in train_loader:\n",
        "      outputD, lossD = train_discriminator(discriminator, generator, imgs)\n",
        "      lossD.backward()\n",
        "      Dtotal_train_loss += lossD.item()\n",
        "      optimizerDisc.step()\n",
        "      optimizerDisc.zero_grad()\n",
        "\n",
        "      outputG, lossG = train_generator(discriminator, generator, imgs)\n",
        "      lossG.backward()\n",
        "      Gtotal_train_loss += lossG.item()\n",
        "      optimizerGen.step()\n",
        "      optimizerGen.zero_grad()\n",
        "      iteration += 1\n",
        "\n",
        "    Gtrain_loss.append(float(Gtotal_train_loss) / (iteration + 1))\n",
        "    #Gval_loss.append(evaluate(model, valid_loader, criterion))\n",
        "    Dtrain_loss.append(float(Dtotal_train_loss) / (iteration + 1))\n",
        "\n",
        "    print((\"Epoch {}: Gen Train loss: {}, Disc Train loss: {}\").format(\n",
        "               # + \"Gen Validation loss: {}, Disc Validation loss: {}\").format(\n",
        "                   epoch + 1,\n",
        "                   #train_acc[epoch],\n",
        "                   Gtrain_loss[epoch],\n",
        "                   Dtrain_loss[epoch]))\n",
        "                   #val_acc[epoch],\n",
        "                   #val_loss[epoch]))\n",
        "    # Save the current model (checkpoint) to a file\n",
        "    #model_path = get_model_name(model.name, learning_rate, epoch)\n",
        "    #torch.save(model.state_dict(), model_path)\n",
        "  print('Finished Training')\n",
        "  end_time = time.time()\n",
        "  elapsed_time = end_time - start_time\n",
        "  print(\"Total time elapsed: {:.2f} seconds\".format(elapsed_time))\n",
        "  #print((\"Final Train Accuracy: {}, |\"+\n",
        "  #             \"Final Validation Accuracy: {}\").format(\n",
        "  #                 train_acc[-1],\n",
        "  #                 val_acc[-1],))\n",
        "#return train_acc, train_loss, val_acc, val_loss\n",
        "  noise = torch.randn(64, 100)\n",
        "  fake_imgs = generator(noise).detach()\n",
        "  genImgs.append(fake_imgs)\n",
        "  return Gtrain_loss, Dtrain_loss, genImgs\n",
        "\n",
        "def plot(Gtrain_loss, Dtrain_loss):\n",
        "    # Plotting\n",
        "    plt.title(\"Gen Train Loss vs Disc Train Loss\")\n",
        "    n = len(Gtrain_loss) # number of epochs\n",
        "    plt.plot(range(1,n+1), Gtrain_loss, label=\"Gen Train\")\n",
        "    plt.plot(range(1,n+1), Dtrain_loss, label=\"Disc Train\")\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.legend(loc='best')\n",
        "    plt.show()\n",
        "\n",
        "def check(dataLoader, genImgs):\n",
        "  real = next(iter(dataLoader))\n",
        "\n",
        "  plt.figure(figsize = (15,15))\n",
        "  plt.subplot(1,2,1)\n",
        "  plt.axis(\"off\")\n",
        "  plt.title(\"Real\")\n",
        "  plt.imshow(np.transpose(real[0], (1,2,0)))\n",
        "\n",
        "  plt.subplot(1,2,2)\n",
        "  plt.axis(\"off\")\n",
        "  plt.title(\"Fake\")\n",
        "  plt.imshow(np.transpose(genImgs[0], (1,2,0)))\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "ifcgk89_SJ6H"
      },
      "outputs": [],
      "source": [
        "generator = Generator()\n",
        "discriminator = Discriminator()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 277
        },
        "id": "arwghmyVoA8g",
        "outputId": "9450b27b-b2a5-4a6d-d52b-66ed81c44e44"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1: Gen Train loss: 0.4923574021812442, Disc Train loss: 0.6784963108599186\n",
            "Epoch 2: Gen Train loss: 0.40044359117746353, Disc Train loss: 0.5424650758504868\n",
            "Epoch 3: Gen Train loss: 0.40030098594725133, Disc Train loss: 0.5426447823643684\n",
            "Epoch 4: Gen Train loss: 0.4002838209271431, Disc Train loss: 0.5425958923995495\n",
            "Epoch 5: Gen Train loss: 0.4003102082759142, Disc Train loss: 0.5425026543438435\n",
            "Epoch 6: Gen Train loss: 0.40032042637467385, Disc Train loss: 0.5424760162830353\n",
            "Epoch 7: Gen Train loss: 0.4003911461681128, Disc Train loss: 0.5425536558032036\n",
            "Epoch 8: Gen Train loss: 0.4014841511845589, Disc Train loss: 0.544950196146965\n",
            "Epoch 9: Gen Train loss: 0.40242870002985, Disc Train loss: 0.5455333597958087\n",
            "Epoch 10: Gen Train loss: 0.4002307254821062, Disc Train loss: 0.5426676839590072\n",
            "Epoch 11: Gen Train loss: 0.40253945626318455, Disc Train loss: 0.5437220118939876\n",
            "Epoch 12: Gen Train loss: 0.3998158920556307, Disc Train loss: 0.543389817327261\n",
            "Epoch 13: Gen Train loss: 0.40111791156232357, Disc Train loss: 0.5422057837247849\n",
            "Epoch 14: Gen Train loss: 0.4004207093268633, Disc Train loss: 0.5427474327385425\n",
            "Epoch 15: Gen Train loss: 0.40020939111709597, Disc Train loss: 0.5426475174725056\n",
            "Epoch 16: Gen Train loss: 0.4003429558128119, Disc Train loss: 0.5425239272415638\n",
            "Epoch 17: Gen Train loss: 0.40034623071551323, Disc Train loss: 0.5425131537020207\n",
            "Epoch 18: Gen Train loss: 0.40034711100161074, Disc Train loss: 0.5425081551074982\n",
            "Epoch 19: Gen Train loss: 0.40042055658996106, Disc Train loss: 0.5424900896847248\n",
            "Epoch 20: Gen Train loss: 0.40039825700223447, Disc Train loss: 0.542513394355774\n",
            "Epoch 21: Gen Train loss: 0.4003026410937309, Disc Train loss: 0.5425264291465283\n",
            "Epoch 22: Gen Train loss: 0.4003437880426645, Disc Train loss: 0.5424946144223213\n",
            "Epoch 23: Gen Train loss: 0.40028428882360456, Disc Train loss: 0.5425237320363522\n",
            "Epoch 24: Gen Train loss: 0.4006208796054125, Disc Train loss: 0.5423158571124077\n",
            "Epoch 25: Gen Train loss: 0.4004358422011137, Disc Train loss: 0.5424736574292183\n",
            "Epoch 26: Gen Train loss: 0.40052533373236654, Disc Train loss: 0.5423872984945775\n",
            "Epoch 27: Gen Train loss: 0.4003385942429304, Disc Train loss: 0.5424997039139271\n",
            "Epoch 28: Gen Train loss: 0.40070256479084493, Disc Train loss: 0.5423052608966827\n",
            "Epoch 29: Gen Train loss: 0.40037118569016455, Disc Train loss: 0.5425515897572041\n",
            "Epoch 30: Gen Train loss: 0.40038553848862646, Disc Train loss: 0.5424844533205032\n",
            "Epoch 31: Gen Train loss: 0.4003630694001913, Disc Train loss: 0.5424795635044575\n",
            "Epoch 32: Gen Train loss: 0.4004081927239895, Disc Train loss: 0.5424609929323196\n",
            "Epoch 33: Gen Train loss: 0.4003443643450737, Disc Train loss: 0.5424786105751991\n",
            "Epoch 34: Gen Train loss: 0.40040204040706157, Disc Train loss: 0.5424518167972565\n",
            "Epoch 35: Gen Train loss: 0.4003821961581707, Disc Train loss: 0.5424560002982617\n",
            "Epoch 36: Gen Train loss: 0.4003825463354588, Disc Train loss: 0.5424544833600521\n",
            "Epoch 37: Gen Train loss: 0.4003940735012293, Disc Train loss: 0.5424476072192193\n",
            "Epoch 38: Gen Train loss: 0.4003803227096796, Disc Train loss: 0.5424599967896938\n",
            "Epoch 39: Gen Train loss: 0.4003764525055885, Disc Train loss: 0.542454157024622\n",
            "Epoch 40: Gen Train loss: 0.40041993744671345, Disc Train loss: 0.5424390517175197\n",
            "Epoch 41: Gen Train loss: 0.40037948116660116, Disc Train loss: 0.5424480922520161\n",
            "Epoch 42: Gen Train loss: 0.40039592497050763, Disc Train loss: 0.5424590423703194\n",
            "Epoch 43: Gen Train loss: 0.4003935005515814, Disc Train loss: 0.5424439527094365\n",
            "Epoch 44: Gen Train loss: 0.4009705100208521, Disc Train loss: 0.5420413218438626\n",
            "Epoch 45: Gen Train loss: 0.40066355653107166, Disc Train loss: 0.542319318652153\n",
            "Epoch 46: Gen Train loss: 0.4007372952997684, Disc Train loss: 0.5422959811985493\n",
            "Epoch 47: Gen Train loss: 0.4006020352244377, Disc Train loss: 0.5423457749187947\n",
            "Epoch 48: Gen Train loss: 0.40073583498597143, Disc Train loss: 0.5421793669462204\n",
            "Epoch 49: Gen Train loss: 0.40143420547246933, Disc Train loss: 0.5417754828929902\n",
            "Epoch 50: Gen Train loss: 0.401004546135664, Disc Train loss: 0.5420738667249679\n",
            "Epoch 51: Gen Train loss: 0.40083508007228374, Disc Train loss: 0.5421203784644604\n",
            "Epoch 52: Gen Train loss: 0.40051714815199374, Disc Train loss: 0.5423001483082771\n",
            "Epoch 53: Gen Train loss: 0.4002453859895468, Disc Train loss: 0.5426681630313397\n",
            "Epoch 54: Gen Train loss: 0.4007331389933825, Disc Train loss: 0.5421827204525471\n",
            "Epoch 55: Gen Train loss: 0.40050545781850816, Disc Train loss: 0.5422582902014256\n",
            "Epoch 56: Gen Train loss: 0.400574903935194, Disc Train loss: 0.5423007756471634\n",
            "Epoch 57: Gen Train loss: 0.4623217646032572, Disc Train loss: 0.5654549591243268\n",
            "Epoch 58: Gen Train loss: 0.43772646859288217, Disc Train loss: 0.5845118403434754\n",
            "Epoch 59: Gen Train loss: 0.4020992245525122, Disc Train loss: 0.5442164648324251\n",
            "Epoch 60: Gen Train loss: 0.4012956291437149, Disc Train loss: 0.5442952405661344\n",
            "Epoch 61: Gen Train loss: 0.40046270489692687, Disc Train loss: 0.5438682071864605\n",
            "Epoch 62: Gen Train loss: 0.40103765167295935, Disc Train loss: 0.543823293223977\n",
            "Epoch 63: Gen Train loss: 0.4010079190135002, Disc Train loss: 0.5432215124368668\n",
            "Epoch 64: Gen Train loss: 0.40074642598628996, Disc Train loss: 0.5429763689637184\n",
            "Epoch 65: Gen Train loss: 0.40065736696124077, Disc Train loss: 0.5435162097215652\n",
            "Epoch 66: Gen Train loss: 0.39992991760373114, Disc Train loss: 0.5437449276447296\n",
            "Epoch 67: Gen Train loss: 0.40171571895480157, Disc Train loss: 0.543274199962616\n",
            "Epoch 68: Gen Train loss: 0.4002771317958832, Disc Train loss: 0.5438890941441059\n",
            "Epoch 69: Gen Train loss: 0.4001247245818377, Disc Train loss: 0.5430588915944099\n",
            "Epoch 70: Gen Train loss: 0.39984931126236917, Disc Train loss: 0.5432426437735558\n",
            "Epoch 71: Gen Train loss: 0.4016217064112425, Disc Train loss: 0.5430041544139386\n",
            "Epoch 72: Gen Train loss: 0.40085220001637933, Disc Train loss: 0.5423267543315887\n",
            "Epoch 73: Gen Train loss: 0.4006306868046522, Disc Train loss: 0.542695265263319\n",
            "Epoch 74: Gen Train loss: 0.40160005204379556, Disc Train loss: 0.5426015920937062\n",
            "Epoch 75: Gen Train loss: 0.4004110135138035, Disc Train loss: 0.5430317617952823\n",
            "Epoch 76: Gen Train loss: 0.4000740941613913, Disc Train loss: 0.5426718041300773\n",
            "Epoch 77: Gen Train loss: 0.40163930915296076, Disc Train loss: 0.5430150620639325\n",
            "Epoch 78: Gen Train loss: 0.40038016103208063, Disc Train loss: 0.5428457409143448\n",
            "Epoch 79: Gen Train loss: 0.4007709600031376, Disc Train loss: 0.5429082922637463\n",
            "Epoch 80: Gen Train loss: 0.4003207340836525, Disc Train loss: 0.543201693892479\n",
            "Epoch 81: Gen Train loss: 0.40001326650381086, Disc Train loss: 0.5436048910021782\n",
            "Epoch 82: Gen Train loss: 0.40068389624357226, Disc Train loss: 0.5425730846822262\n",
            "Epoch 83: Gen Train loss: 0.40020750276744366, Disc Train loss: 0.5432813599705696\n",
            "Epoch 84: Gen Train loss: 0.4005723990499973, Disc Train loss: 0.543458403646946\n",
            "Epoch 85: Gen Train loss: 0.40092831626534464, Disc Train loss: 0.5419126793742179\n",
            "Epoch 86: Gen Train loss: 0.4008365012705326, Disc Train loss: 0.5433555442839861\n",
            "Epoch 87: Gen Train loss: 0.40056929290294646, Disc Train loss: 0.5431568011641502\n",
            "Epoch 88: Gen Train loss: 0.40025875754654405, Disc Train loss: 0.5436281889677048\n",
            "Epoch 89: Gen Train loss: 0.40182601027190684, Disc Train loss: 0.5414681009948253\n",
            "Epoch 90: Gen Train loss: 0.40082225054502485, Disc Train loss: 0.5424658074975014\n",
            "Epoch 91: Gen Train loss: 0.399839099869132, Disc Train loss: 0.5429149337112904\n",
            "Epoch 92: Gen Train loss: 0.4009292360395193, Disc Train loss: 0.543005183339119\n",
            "Epoch 93: Gen Train loss: 0.40018762424588206, Disc Train loss: 0.5436615779995918\n",
            "Epoch 94: Gen Train loss: 0.4008825533092022, Disc Train loss: 0.5432492084801197\n",
            "Epoch 95: Gen Train loss: 0.40136102922260763, Disc Train loss: 0.5444078270345927\n",
            "Epoch 96: Gen Train loss: 0.4004421629011631, Disc Train loss: 0.5439497180283069\n",
            "Epoch 97: Gen Train loss: 0.4005133606493473, Disc Train loss: 0.5424813516438007\n",
            "Epoch 98: Gen Train loss: 0.4009281575679779, Disc Train loss: 0.5434972740709781\n",
            "Epoch 99: Gen Train loss: 0.4008996389806271, Disc Train loss: 0.5431432481855154\n",
            "Epoch 100: Gen Train loss: 0.4010883282870054, Disc Train loss: 0.543651370704174\n",
            "Epoch 101: Gen Train loss: 0.40019913390278816, Disc Train loss: 0.5434589624404907\n",
            "Epoch 102: Gen Train loss: 0.4009219978004694, Disc Train loss: 0.5427252627909184\n",
            "Epoch 103: Gen Train loss: 0.40042919255793097, Disc Train loss: 0.5434477873146534\n",
            "Epoch 104: Gen Train loss: 0.4007694344967604, Disc Train loss: 0.5425840362906456\n",
            "Epoch 105: Gen Train loss: 0.40098592154681684, Disc Train loss: 0.5433253373950719\n",
            "Epoch 106: Gen Train loss: 0.4004644017666578, Disc Train loss: 0.5435409586876631\n",
            "Epoch 107: Gen Train loss: 0.4006567567586899, Disc Train loss: 0.5451533202081918\n",
            "Epoch 108: Gen Train loss: 0.4013674080371857, Disc Train loss: 0.5437075890600681\n",
            "Epoch 109: Gen Train loss: 0.40112640559673307, Disc Train loss: 0.5431723799556494\n",
            "Epoch 110: Gen Train loss: 0.40024610348045825, Disc Train loss: 0.5448808364570141\n",
            "Epoch 111: Gen Train loss: 0.4050745747983456, Disc Train loss: 0.544812211021781\n",
            "Epoch 112: Gen Train loss: 0.4013684865087271, Disc Train loss: 0.5443063892424107\n",
            "Epoch 113: Gen Train loss: 0.400321214273572, Disc Train loss: 0.5434910289943218\n",
            "Epoch 114: Gen Train loss: 0.400720114633441, Disc Train loss: 0.5444714326411486\n",
            "Epoch 115: Gen Train loss: 0.4010737434029579, Disc Train loss: 0.5457562152296305\n",
            "Epoch 116: Gen Train loss: 0.4002807930111885, Disc Train loss: 0.5428642101585865\n",
            "Epoch 117: Gen Train loss: 0.40190829858183863, Disc Train loss: 0.5442832060158252\n",
            "Epoch 118: Gen Train loss: 0.40217095911502837, Disc Train loss: 0.5503173984587193\n"
          ]
        }
      ],
      "source": [
        "Gtrain_loss, Dtrain_loss, genImgs = train(discriminator, generator, 0.01, 500, torch.utils.data.DataLoader(mnist_train, batch_size=128))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAAsTAAALEwEAmpwYAAAXxUlEQVR4nO3de2zV5f0H8PdHLlpoUcq1VAK4oImBieYMSdhQsmgETfAeFTeNDFCBgCBUBZyRm3ITB1hbQO2myFzmFC9Z5MfmdBovlSCgjB+CxUHLpSIIonL7/P7o8ZdO+3w+9Zz2nJM971dCgPPu0z582w+nPZ/v8zyiqiCi/36nZHsCRJQZLHaiSLDYiSLBYieKBIudKBItM/nB8vPztbCwMJh37tw55fftdRVExMxramrMPC8vL5h9/fXX5tiOHTua+cmTJ818//79Zm5p0aKFmZ84ccLMvetaVFRk5tZ1r66uNse2atXKzDt16mTm3ufU4n3OTjnFfp70rmvLluHS27Nnjzm2a9euwWzHjh2ora1t8KKnVewichmARwG0ALBcVR+y3r6wsBBTpkwJ5nfeeaf38YKZd3G9T87s2bPN/Kc//WkwW79+vTl29OjRZn7kyBEzf+aZZ8zcKsh27dqZYw8ePJjy+waAkpISM2/dunUwmzFjhjnW+8/fu64PPRT+cvT+gx05cqSZW/8uADh8+LCZW096ixYtMsdOmjQpmA0cODCYpfxtvIi0ALAUwBAA5wK4UUTOTfX9EVHzSudn9v4APlHV7ap6FMAqAMOaZlpE1NTSKfZiAP+u9/edycf+g4iMEpFKEan0vrUhoubT7K/Gq2q5qiZUNZGfn9/cH46IAtIp9l0Autf7+5nJx4goB6VT7O8D6C0ivUSkNYAbAKxummkRUVOTdFa9ichQAItQ13p7QlVnWW+fSCT0vffeC+bez/QrV64MZrW1tebYO+64w8y99tYXX3xh5pbp06ebudc29HrllgcffNDM77//fjP32mNeS7NNmzZmbvnqq6/M3OvDDxgwIJglEglzrDfvOXPmmPmpp55q5t98800w89qCVttu3rx5+Oyzz5q+z66qrwJ4NZ33QUSZwdtliSLBYieKBIudKBIsdqJIsNiJIsFiJ4pERteze6wePAAMHz48mFnrzQFg69atZj527Fgzf/jhh4OZt8zT4/Vsp02bZubl5eXBzOvRL1u2zMzbt29v5t79BxMmTAhm3lJOb+7edX/ssceC2ZtvvmmOLSgoMPNx48aZ+ZIlS8z83nvvDWbe3guLFy8OZtZ9M3xmJ4oEi50oEix2okiw2IkiwWInigSLnSgSaS1x/bGKi4vV2hF04sSJ5vgXXnghmA0ePNgc+9Zbb5n55s2bzdxqfz3//PPm2GuuucbMjx8/bubr1q0z83feeSeYjR8/3hzrbYPttce85bmnnXZaMPv222/Nsd4OrnfffbeZz507N5h58x4xYoSZL1++3My9ZarWVtLHjh0zx1otydLSUuzatavB3h2f2YkiwWInigSLnSgSLHaiSLDYiSLBYieKBIudKBIZXeJaVFRk9qtnzpxpjrd6tm3btjXHbtmyxcy9ZYXWlsn/+te/zLHWaaIAcM4555j5z372MzO3WEtzAb+n6/WL77vvvpQ//pgxY8yx1lJOANi3b5+ZW71073Qi77hnr09vLe0FgL/+9a/B7PLLLzfHrlixIphZPXg+sxNFgsVOFAkWO1EkWOxEkWCxE0WCxU4UCRY7USQyvpW0tX7e2zr4yJEjwcxbl+2xjsEF7HsAvF60l1955ZVm7m01bV23o0ePmmOtexcAv5+8YMECM7/ooouC2dKlS82x3r0Pjz/+uJlbRzr379/fHPvKK6+YuXfUtXfdqqurg5l3TS+44IJgZh0VnVaxi0gVgEMATgA4rqr2oddElDVN8cw+WFVrm+D9EFEz4s/sRJFIt9gVwGsi8oGIjGroDURklIhUikildy8zETWfdIv956p6AYAhAMaIyKDvv4GqlqtqQlUTnTp1SvPDEVGq0ip2Vd2V/H0vgL8AsF/iJKKsSbnYRaStiBR892cAlwLY1FQTI6KmlfK+8SJyFuqezYG6V/VXquosa0y3bt3MfeNvu+0282P+6U9/Cmbe+uTdu3ebucc6Ytdj7REOALNnzzZzr2dr9Va9ddVeT9f72D179jTzqqqqYJbumQXWHgPe+/fufbjjjjvM3Lsvw7s3wrr/4NNPPzXHWte0rKwM1dXVDd6gkHLrTVW3Azgv1fFElFlsvRFFgsVOFAkWO1EkWOxEkWCxE0Uio0tcW7VqhaKiomD+5JNPmuOtbajnz5+f8rwaw9oSOd0Wktea85Z6Wkcfe2O9FpQ3/vDhw2Zutce89pa3hNVjfV7y8vLMsRUVFWY+duxYM/eu68CBA4PZeefZTa5HH300mFmfLz6zE0WCxU4UCRY7USRY7ESRYLETRYLFThQJFjtRJDLaZz9x4gT2798fzEtKSszx1rJB7+hgrxd+4MABM1+yZEkwmzp1qjnWO7L5+PHjZu714a1e9iOPPGKO9XYPsj5fAHDw4EEzt/rNpaWl5tiJEyeaubc812JtSw4AkyZNMnPvvg5ve/CvvvoqmJWVlZljra9lK+MzO1EkWOxEkWCxE0WCxU4UCRY7USRY7ESRYLETRSKjfXZVxbFjx4L56tWrzfFWz9ZbPzxrlrnLNXr16mXmVi993rx55lhvG+oZM2aYudeHt9x1111mbq2FB4B169aZ+RtvvGHm1vrqzp07m2O9Xrb3ObeOo7a+DgH//oSCggIz79u3r5lb24d794xY6/ytY6r5zE4UCRY7USRY7ESRYLETRYLFThQJFjtRJFjsRJHIaJ9dRMy12VdddZU53uqVez16r9/8+eefm/nChQuD2ZQpU8yx3vG93nr1dPZ293rVXg+/T58+Zu71m6195b1r7unQoYOZW2vGhw8fbo59+umnzdzbH8Hr4w8ZMiTl933dddcFs/Ly8mDmPrOLyBMisldENtV7rFBE1ojI1uTv7b33Q0TZ1Zhv458CcNn3HrsHwFpV7Q1gbfLvRJTD3GJX1TcAfH9vomEAvjsfpwLAlU07LSJqaqm+QNdFVWuSf94NoEvoDUVklIhUikil9TMUETWvtF+N17pXE4KvKKhquaomVDXRtm3bdD8cEaUo1WLfIyJFAJD8fW/TTYmImkOqxb4awC3JP98C4MWmmQ4RNRe3zy4izwK4GEBHEdkJ4LcAHgLwnIiMALADwPWN+WCqihMnTgRzrx99//33B7PFixebY99++20zf/fdd8188uTJwczri1r/ZsBf7271+AG7z+6tV/fOX9+0aZOZe+MtXo/fWpsNpLdn/apVq8yx7dq1M/Prr7e/5L09DqwzEryx1tdbbW1tMHOLXVVvDES/9MYSUe7g7bJEkWCxE0WCxU4UCRY7USRY7ESRyOgS16KiInOb3JkzZ5rjd+zYEcy8Nsyll15q5pWVlWZ+6qmnBjNv3p65c+eaec+ePc18+/btwcxrC1rHPQNAXl6emXutvW7dugWznTt3mmO9I5u9o7Bbt24dzLx/95gxY8zca921aNHCzP/2t78FM2/JtLX1uNVu5DM7USRY7ESRYLETRYLFThQJFjtRJFjsRJFgsRNFIqN99urqarNH6PU+zzzzzGDmbbfsLTMdNGiQmVvz9nqq6c7tmmuuMXOvT2/xjqretm2bmY8bN87Mly5dGszatGljji0rKzNzq48O2Nf19ttvN8d6RzZfcsklZm5t9wzYy1g/+ugjc2zXrl2DGY9sJiIWO1EsWOxEkWCxE0WCxU4UCRY7USRY7ESRyPiRzVYfcMSIEeZ4r5+dztiamhozt7ZM7tGjhzm2qqrKzD1eH926pvn5+ebYfv36mfn+/d8/5u8/HT161Mz79u0bzLxe9YIFC8zcuy/jN7/5TTBbsmSJOfbXv/61mVdUVJi59zVh3XvhHZN26NChYGYdFc1ndqJIsNiJIsFiJ4oEi50oEix2okiw2IkiwWInioR4+4o3pW7duuno0aOD+WWXXWaO79+/fzDbvXu3Ofb0008385dfftnMr7766mDm7V/urWdv2dK+3cFb727taZ/ukc3p7jtvjfc+tnfdJk2aZObz589PaV6AP7cOHTqY+eeff27mp512WjDz/l2vv/56MLvzzjuxZcuWBifvPrOLyBMisldENtV77AER2SUi65O/hnrvh4iyqzHfxj8FoKGn3EdUtV/y16tNOy0iampusavqGwDseyaJKOel8wLdWBHZkPw2v33ojURklIhUikjlkSNH0vhwRJSOVIu9FMBPAPQDUAMguGJBVctVNaGqCW+DQSJqPikVu6ruUdUTqnoSwDIA4ZfJiSgnpFTsIlJU769XAdgUelsiyg3uenYReRbAxQA6ishOAL8FcLGI9AOgAKoAhJvnP8IXX3xh5g8//HAw89ZVe3uMT5gwwcytfnI6vebG5F26dDFz6x6DdPvohYWFZn7DDTeYeWlpaTCz1uEDwF133WXm3v0N06ZNC2YzZ840x5599tlmPmTIEDO37n0A7K9lKwOAoUPDnW5r3wa32FX1xgYeXuGNI6LcwttliSLBYieKBIudKBIsdqJIsNiJIpHRJa7FxcVqHZU7depUc7y1tfAZZ5xhjrW2FQb87Xu9bY0tXtvPaxt6S2Ct5ZLev8trMW3dutXMvdZeXl5eMPNun7b+XYC9bTIAHD9+PJh5R02vXLnSzBOJhJmvWbPGzK3Pqbd8trq6OpiVlZWhuro6tSWuRPTfgcVOFAkWO1EkWOxEkWCxE0WCxU4UCRY7USQyemRzq1atUFxcHMx37Nhhjv/mm2+C2U033WSOnTVrlpl7vfDJkycHs4ULF5pjrX4vYPeiAb+fbN1j4PXZhw0bZuavvfaamV9++eVmbi3X9O7x8LbB9nrlixYtCmYFBQXmWOt+EMBfIjt9+nQzt+5PWL58uTnWWj5rLsU23ysR/ddgsRNFgsVOFAkWO1EkWOxEkWCxE0WCxU4UiYyuZ08kEvree++lPH7OnDnBzOtle2vC0znC13vf3np1jze3rl27BrO9e/eaY725jxo1ysyXLl1q5hZr22PAP7LZy7318Jbx48ebubcNtndfh/X15B3Rbe37cOGFF+KDDz7genaimLHYiSLBYieKBIudKBIsdqJIsNiJIsFiJ4pERtezA3Z/0Vt77fUfLX379jXzjz/+2Mw7d+4czGpqasyx6d7L0K5dOzOvra0NZun2qq09yhvDugfA+9jeEd7e/QvW/gfWcc6A/7X23HPPmXlJSYmZW+bNm2fm77zzTjCzash9ZheR7iLydxH5WEQ+EpHxyccLRWSNiGxN/t7ee19ElD2N+Tb+OIBJqnougAEAxojIuQDuAbBWVXsDWJv8OxHlKLfYVbVGVdcl/3wIwGYAxQCGAahIvlkFgCubaY5E1AR+1At0ItITwPkA3gXQRVW/+2F1N4AugTGjRKRSRCr37duXzlyJKA2NLnYRyQfwZwATVPXL+pnWvQLV4KtQqlquqglVTXTq1CmtyRJR6hpV7CLSCnWF/oyqPp98eI+IFCXzIgD28ioiyiq39SZ1vbIVADarav09k1cDuAXAQ8nfX/Tel6qa2wNbrRLAXhLpLXE9dOiQOzfLgQMHgpl3bLG1vS/gt6C8f5s3Pp333bFjRzP3lshaS2y9eXufE++6eu1WS2lpqZkPHz7czB977DEzt74evaPLn3zyyWBm1VBj+uwDAfwKwEYRWZ987D7UFflzIjICwA4A1zfifRFRlrjFrqr/BBB66vpl006HiJoLb5cligSLnSgSLHaiSLDYiSLBYieKREa3kj7//PP1H//4RzCvqKgIZgCQSCSC2YABA8yxs2fPtifnsI4mfvnll82x6WxT3ZjcWo7pbXnsHQft8Y66tubmXZd087POOiuYbdu2zRx73nnnmfkVV1xh5p6nnnoqmN12223m2J07dwazK664Ahs2bOBW0kQxY7ETRYLFThQJFjtRJFjsRJFgsRNFgsVOFImM9tmLi4t19OjRwXzs2LHm+FdeeSWY3XzzzSnPCwDWr19v5qtXrw5m3rrqHj16mHlVVZWZT5w40cwXL14czLw+urce3dtS2evj33333cHMu/fB20L7F7/4hZlbXy/ecc5ff/21mY8ZM8bMvevyu9/9Lpilc39BWVkZqqur2WcnihmLnSgSLHaiSLDYiSLBYieKBIudKBIsdqJIZPzIZou1ThcAPv3002A2Y8YMc6y3z7fVk/V4+3x7R1Fv3LjRzMvLy83c6rsOGjTIHPvhhx+aubfffnFxsZmvWrUqmLVt29Yce+2115r52rVrzdzq03v3dHj3AHj7yt96661mfsYZZwSzgwcPmmN79+4dzPLy8oIZn9mJIsFiJ4oEi50oEix2okiw2IkiwWInigSLnSgSjTmfvTuA3wPoAkABlKvqoyLyAICRAPYl3/Q+VX3V/GAtW6Jz587B/PXXXzfnks7e7b169TJzb/9za31zdXW1OXbFihVm7u0L761vnj59ejBbuHChOfbo0aNm7vHW4lv3IHhr5efMmWPm3j4C1sf27ssYNmyYmffp08fMlyxZYua33357MHv66afNsYMHDw5mBQUFwawxN9UcBzBJVdeJSAGAD0RkTTJ7RFXnN+J9EFGWNeZ89hoANck/HxKRzQDs26aIKOf8qJ/ZRaQngPMBvJt8aKyIbBCRJ0SkfWDMKBGpFJHKw4cPpzdbIkpZo4tdRPIB/BnABFX9EkApgJ8A6Ie6Z/4FDY1T1XJVTahqIj8/P/0ZE1FKGlXsItIKdYX+jKo+DwCqukdVT6jqSQDLAPRvvmkSUbrcYpe6l4pXANisqgvrPV5U782uArCp6adHRE2lMa/GDwTwKwAbRWR98rH7ANwoIv1Q146rAhDeIzqpoKAAF110UTB/8cUXzfGnn356MPPaVy+99JKZT5482cytJY9WOxHwl4HW1NSY+bRp08x85syZwSzdrcK7du1q5vv37zdz67rv3bvXHOvNvU2bNmb+xz/+MZhZLSoA6Nu3r5lv377dzL2tpK1Wr7f997Jly4LZvn37glljXo3/J4CGKsnsqRNRbuEddESRYLETRYLFThQJFjtRJFjsRJFgsRNFIqNHNnfv3l2t44e//PJLc7y1/e6BAwfMsd5yyHvvvdfMrT57uv3gb7/91sy9paDWlsxHjhwxx3rXZcqUKWbuLaG1+tkjR440x7Zo0cLMvWWk48aNC2azZs0yx6azrBgATp48aebWfSHe2BdeeCGYlZSUYNu2bTyymShmLHaiSLDYiSLBYieKBIudKBIsdqJIsNiJIpHRPruI7AOwo95DHQHUZmwCP06uzi1X5wVwbqlqyrn1UNVODQUZLfYffHCRSlVNZG0ChlydW67OC+DcUpWpufHbeKJIsNiJIpHtYi/P8se35OrccnVeAOeWqozMLas/sxNR5mT7mZ2IMoTFThSJrBS7iFwmIltE5BMRuScbcwgRkSoR2Sgi60WkMstzeUJE9orIpnqPFYrIGhHZmvy9wTP2sjS3B0RkV/LarReRoVmaW3cR+buIfCwiH4nI+OTjWb12xrwyct0y/jO7iLQA8L8ALgGwE8D7AG5U1Y8zOpEAEakCkFDVrN+AISKDABwG8HtV7ZN8bC6A/ar6UPI/yvaqWpIjc3sAwOFsH+OdPK2oqP4x4wCuBHArsnjtjHldjwxct2w8s/cH8ImqblfVowBWARiWhXnkPFV9A8D3j1wZBqAi+ecK1H2xZFxgbjlBVWtUdV3yz4cAfHfMeFavnTGvjMhGsRcD+He9v+9Ebp33rgBeE5EPRGRUtifTgC6q+t15UbsBdMnmZBrgHuOdSd87Zjxnrl0qx5+niy/Q/dDPVfUCAEMAjEl+u5qTtO5nsFzqnTbqGO9MaeCY8f+XzWuX6vHn6cpGse8C0L3e389MPpYTVHVX8ve9AP6C3DuKes93J+gmf7dPR8ygXDrGu6FjxpED1y6bx59no9jfB9BbRHqJSGsANwBYnYV5/ICItE2+cAIRaQvgUuTeUdSrAdyS/PMtAOyjbzMoV47xDh0zjixfu6wff66qGf8FYCjqXpHfBmBqNuYQmNdZAD5M/voo23MD8Czqvq07hrrXNkYA6ABgLYCtAP4HQGEOze0PADYC2IC6wirK0tx+jrpv0TcAWJ/8NTTb186YV0auG2+XJYoEX6AjigSLnSgSLHaiSLDYiSLBYieKBIudKBIsdqJI/B8enWQHico8zQAAAABJRU5ErkJggg==",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torchvision\n",
        "\n",
        "\n",
        "def imshow(inp, title=None):\n",
        "    \"\"\"Display image for Tensor.\"\"\"\n",
        "    inp = inp.numpy().transpose((1, 2, 0))\n",
        "    mean = np.array([0.5, 0.5, 0.5])\n",
        "    std = np.array([0.5, 0.5, 0.5])\n",
        "    inp = std * inp + mean\n",
        "    inp = np.clip(inp, 0, 1)\n",
        "    plt.imshow(inp)\n",
        "    if title is not None:\n",
        "        plt.title(title)\n",
        "    plt.pause(0.001)  # pause a bit so that plots are updated\n",
        "\n",
        "imshow(genImgs[0][0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
