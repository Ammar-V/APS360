{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "########################################\n",
    "TODOS:\n",
    "- Mapping Network\n",
    "- Generator Network\n",
    "- Discriminator Network\n",
    "- Augmentation/RGBBlock (style vector to RGB)\n",
    "\n",
    "\n",
    "\n",
    "########################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchsummary\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "import os\n",
    "import math\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################\n",
    "### SETTINGS\n",
    "##########################\n",
    "\n",
    "# Device\n",
    "CUDA_DEVICE_NUM = 0\n",
    "DEVICE = torch.device(f'cuda:{CUDA_DEVICE_NUM}' if torch.cuda.is_available() else 'cpu')\n",
    "print('Device:', DEVICE)\n",
    "\n",
    "# Hyperparameters\n",
    "RANDOM_SEED = 42\n",
    "GENERATOR_LEARNING_RATE = 0.0002\n",
    "DISCRIMINATOR_LEARNING_RATE = 0.0002\n",
    "\n",
    "NUM_EPOCHS = 200\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "IMAGE_HEIGHT, IMAGE_WIDTH, IMAGE_CHANNELS = 256, 256, 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt # for plotting\n",
    "import numpy as np\n",
    "import torch\n",
    "import math\n",
    "import os\n",
    "import glob\n",
    "\n",
    "transform = transforms.Compose([\n",
    "            # transforms.RandomHorizontalFlip(), # Flip the data horizontally\n",
    "            transforms.ToPILImage(),\n",
    "            transforms.Resize((256, 256)),\n",
    "            # transforms.RandomAdjustSharpness(0.25),\n",
    "            # transforms.RandomHorizontalFlip(0.5),\n",
    "            # transforms.RandomVerticalFlip(0.5),\n",
    "            transforms.ToTensor(),\n",
    "            # transforms.Normalize(mean=(0.5,0.5,0.5), std=(0.5,0.5,0.5))\n",
    "        ])\n",
    "\n",
    "# Load the data with all of the classes\n",
    "root = '..\\\\data\\\\'\n",
    "data_dirs = glob.glob(root + '**/*.jpg', recursive=True)\n",
    "data_dirs = np.array(data_dirs)\n",
    "data_dirs = data_dirs.flatten()\n",
    "\n",
    "roots = np.array([root] * len(data_dirs))\n",
    "data_dirs = np.core.defchararray.add(roots, data_dirs)\n",
    "\n",
    "\n",
    "# data_list = np.array(data.imgs)\n",
    "\n",
    "# Get the indices for the train/val/test split of 75/15/15\n",
    "train_split = math.floor(1 * len(data_dirs))\n",
    "val_split = math.ceil(0.00 * len(data_dirs))\n",
    "test_split = val_split * 0\n",
    "\n",
    "# print(train_split + val_split + test_split, len(data_arr))\n",
    "\n",
    "# Make sure the splits are correct\n",
    "assert train_split + val_split + test_split == len(data_dirs)\n",
    "\n",
    "# Split the dataset randomely\n",
    "generator = torch.Generator().manual_seed(1)\n",
    "train, val, test = torch.utils.data.random_split(data_dirs, [train_split, val_split, test_split], generator=generator)\n",
    "\n",
    "len(train), len(val), len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "class GenData(torch.utils.data.Dataset):\n",
    "    '''\n",
    "        Data set class to store the feature maps\n",
    "    \n",
    "    '''\n",
    "\n",
    "    def __init__(self, in_data, transform=None):\n",
    "        # data = np.array(in_data)\n",
    "        self.input_dirs = in_data\n",
    "        # self.labels = data[:, 1]\n",
    "        self.labels = []\n",
    "        self.input_transform = transform[0]\n",
    "        self.transform = transform[1]\n",
    "        self.resize_transform = transforms.Compose([\n",
    "                                    transforms.ToPILImage(),\n",
    "                                    transforms.Resize(128)\n",
    "                                ])\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_dirs)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        input_dir = self.input_dirs[idx]\n",
    "\n",
    "        # Load the data\n",
    "        inputs = cv2.imread(input_dir)\n",
    "        inputs = cv2.cvtColor(inputs, cv2.COLOR_BGR2RGB)\n",
    "        # inputs = inputs.swapaxes(0, 2)\n",
    "\n",
    "        # labels = (int)(self.labels[idx])\n",
    "\n",
    "        if self.transform:\n",
    "            inputs = self.input_transform(inputs)\n",
    "            transformed = self.transform(inputs)\n",
    "\n",
    "            # labels = self.input_transform(labels)\n",
    "\n",
    "            return inputs, inputs\n",
    "        else:\n",
    "            return inputs, inputs, inputs\n",
    "        \n",
    "# Create data loaders\n",
    "def get_data_loaders(batch_size=1):\n",
    "    \n",
    "    train_data = GenData(data_dirs[train.indices], transform=(transform, transform))\n",
    "    val_data = GenData(data_dirs[val.indices], transform=(transform, transform))\n",
    "    test_data = GenData(data_dirs[test.indices], transform=(transform, transform))\n",
    "\n",
    "    train_dataloader = torch.utils.data.DataLoader(train_data, batch_size=batch_size)\n",
    "    val_dataloader = torch.utils.data.DataLoader(val_data, batch_size=batch_size)\n",
    "    test_dataloader = torch.utils.data.DataLoader(test_data, batch_size=batch_size)\n",
    "\n",
    "    return train_dataloader, val_dataloader, test_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################\n",
    "### Dataset\n",
    "##########################\n",
    "\n",
    "\n",
    "custom_transforms = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.CenterCrop((160, 160)),\n",
    "    torchvision.transforms.Resize([IMAGE_HEIGHT, IMAGE_WIDTH]),\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "\n",
    "train_loader, valid_loader, test_loader = get_data_loaders(\n",
    "    batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the dataset\n",
    "\n",
    "    \n",
    "# Checking the dataset\n",
    "print('Validation Set:\\n')\n",
    "for images, labels in valid_loader:  \n",
    "    print('Image batch dimensions:', images.size())\n",
    "    print('Image label dimensions:', labels.size())\n",
    "    #print(labels[:10])\n",
    "    break\n",
    "\n",
    "# Checking the dataset\n",
    "print('\\nTesting Set:')\n",
    "for images, labels in test_loader:  \n",
    "    print('Image batch dimensions:', images.size())\n",
    "    print('Image label dimensions:', labels.size())\n",
    "    #print(labels[:10])\n",
    "    break\n",
    "\n",
    "print('\\nTraining Set:')\n",
    "for images, labels in train_loader:  \n",
    "    print('Image batch dimensions:', images.size())\n",
    "    print('Image label dimensions:', labels.size())\n",
    "    #print(labels[:10])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 8))\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Source Images\")\n",
    "plt.imshow(np.transpose(torchvision.utils.make_grid(images[:64], \n",
    "                                         padding=2, normalize=False),\n",
    "                        (1, 2, 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EqualizedWeight(nn.Module):\n",
    "    def __init__(self, shape):\n",
    "        super().__init__()\n",
    "        self.constant = 1 / math.sqrt(np.prod(shape[1:]))\n",
    "        self.weight = nn.Parameter(torch.randn(shape))\n",
    "\n",
    "    def forward(self):\n",
    "        return self.weight * self.constant\n",
    "\n",
    "class UpsampleBlock(nn.Module):\n",
    "    '''\n",
    "    Block that upsamples image in the Synthesis Network\n",
    "    ie: 4x4 -> 8x8\n",
    "\n",
    "    Test: Done\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.upsample = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False)\n",
    "\n",
    "        self.kernel = torch.tensor([[[[1, 2, 1],[2, 4, 2],[1, 2, 1]]]],dtype=torch.float).cuda()\n",
    "\n",
    "        self.kernel /= self.kernel.sum()\n",
    "        self.pad = nn.ReflectionPad2d(1)\n",
    "        self.conv2d = F.conv2d\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, h, w = x.shape\n",
    "        x = self.upsample(x)\n",
    "        x = x.view(-1, 1, h, w)\n",
    "        x = self.pad(x)\n",
    "        x = self.conv2d(x, self.kernel)\n",
    "\n",
    "        return x.view(b, c, h * 2, w * 2)\n",
    "\n",
    "class ConvModDeMod(nn.Module):\n",
    "    def __init__(self, in_features, out_features, kernel_size, demodulate):\n",
    "        super().__init__()\n",
    "        self.demodulate = demodulate\n",
    "        self.out_features = out_features\n",
    "\n",
    "        self.padding = (kernel_size - 1) // 2\n",
    "\n",
    "        self.weight = EqualizedWeight([out_features, in_features, kernel_size, kernel_size])\n",
    "\n",
    "        self.eps = 1e-8\n",
    "\n",
    "    def forward(self, x, style_vector):\n",
    "        b, c, h, w = x.shape\n",
    "        style_vector = style_vector[:, None, :, None, None]\n",
    "\n",
    "        weights = self.weight()[None, :, :, :, :]\n",
    "\n",
    "        # Modulation step\n",
    "        weights *= style_vector\n",
    "\n",
    "        if self.demodulate:\n",
    "            sigma_inv = torch.rsqrt((weights ** 2).sum(dim=(2, 3, 4), keepdim=True) + self.eps)\n",
    "\n",
    "            weights *= sigma_inv\n",
    "\n",
    "        x = x.reshape(1, -1, h, w)\n",
    "        _, _, h, w = weights.shape\n",
    "\n",
    "        weights = weights.reshape(b * self.out_features, h, w)\n",
    "\n",
    "        x = F.conv2d(x, weights, padding=self.padding, groups=b)\n",
    "\n",
    "        x = x.reshape(-1, self.out_feautres, h, w)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class StyleBlock(nn.Module):\n",
    "    def __init__(self, w_size, in_features, out_features):\n",
    "        super().__init__()\n",
    "\n",
    "        self.to_style = nn.Linear(w_size, in_features)\n",
    "        self.conv = ConvModDeMod(in_features, out_features, 3)\n",
    "        self.scale_noise = nn.Parameter(torch.zeros(1))\n",
    "        self.bias = nn.Parameter(torch.zeros(out_features))\n",
    "\n",
    "\n",
    "        self.activation = nn.LeakyReLU(0.2, True)\n",
    "\n",
    "    def forward(self, x, w, noise):\n",
    "\n",
    "        s = self.to_style(w)\n",
    "        x = self.conv(x, s)\n",
    "\n",
    "        if noise is not None:\n",
    "            x = x + self.scale_noise[None, :, None, None] * noise\n",
    "        x = self.activation(x + self.bias[None, :, None, None])\n",
    "\n",
    "        return x\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, batch_size=16, w_size=512):\n",
    "        super().__init__()\n",
    "\n",
    "        self.sizes = [4, 8, 16, 32, 64, 128, 256]\n",
    "        self.features = [512, 512, 512, 512, 256, 128, 64]\n",
    "\n",
    "        self.init_noise = torch.randn([batch_size, w_size, 4, 4])\n",
    "\n",
    "        self.style_block = StyleBlock(w_size, self.features[0], self.features[0])\n",
    "        self.to_rgb = None\n",
    "\n",
    "        blocks = None\n",
    "        self.blocks = nn.ModuleList(blocks)\n",
    "\n",
    "        self.upsample = UpsampleBlock()\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 8, 8])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = torch.rand([1, 3, 4, 4]).cuda()\n",
    "upsample = UpsampleBlock().cuda()\n",
    "out = upsample(test)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "\n",
    "        self.features = [64, 128, 256, 512, 512, 512]\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
